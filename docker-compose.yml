version: "3.8"

# Docker Compose file for optional local services (e.g., Ollama). All runtime
# configuration (including `LLM_PROVIDER`) is picked up from the project-root
# `.env` file â€” nothing is hard-coded here.

services:
  # Local Ollama server (only used when LLM_PROVIDER=ollama)
  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile.app
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    tty: true
    restart: unless-stopped
    pull_policy: always
    profiles:
      - ollama   # Start with:  COMPOSE_PROFILES=ollama docker compose up

  # No Gradio frontend; Next.js app runs separately.

volumes:
  ollama:
